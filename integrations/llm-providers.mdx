---
title: "Anthropic, Cohere, etc."
---

Monitor all LLM providers via LangChain simple unified interface `init_chat_model`:

<CodeGroup>
```py Python
from langchain.chat_models import init_chat_model
from literalai import LiteralClient

lai = LiteralClient()
gpt_4o = init_chat_model("gpt-4o", model_provider="openai", temperature=0)
claude_opus = init_chat_model("claude-3-opus-20240229", model_provider="anthropic", temperature=0)

# Literal AI callback
cb = lai.langchain_callback()

# Invoke the model with input and callback configuration
gpt_4o.invoke("what's your name", config=RunnableConfig(callbacks=[cb]))
claude_opus.invoke("what's your name", config=RunnableConfig(callbacks=[cb]))

lai.flush_and_stop()
```


```typescript TypeScript
import { LiteralClient } from "@literalai/client";

import { HumanMessage } from "@langchain/core/messages";
import { ChatAnthropic } from "@langchain/anthropic";

const literal_client = new LiteralClient(process.env.LITERAL_API_KEY);
const cb = literal_client.instrumentation.langchain.literalCallback();

// Example of using the callback
const message = new HumanMessage({ content: "What is the capital of France?" });

const model = new ChatAnthropic({
  anthropicApiKey: process.env.ANTHROPIC_API_KEY,
});
model.invoke([message], {
  callbacks: [cb],
}).then(response => {
  console.log(response)
}).catch(error => {
  // Handle any errors here
});
```
</CodeGroup>

You can thus monitor Anthropic, Mistral AI, Cohere and many other LLM providers. Full list [here](https://python.langchain.com/docs/integrations/platforms/).

## Link prompts to logged generations

The above code allows you to log all LLM calls. Now, let's connect it to prompt capabilities, creating a virtuous cycle.

1. Save the prompt template on Literal AI 
<Frame caption="Prompt Template">
  <img
    src="/images/template-prompt.png"
    alt="Prompt Template"
  />
</Frame>

2. Run the code to pull the prompt and iterate from there

```python Python
from literalai import LiteralClient
from langchain.schema.runnable.config import RunnableConfig
from langchain.chat_models import init_chat_model
from dotenv import load_dotenv
load_dotenv()

lai = LiteralClient()  # Initialize LiteralClient

# Get prompt containing: prompt template, model provider, and model settings
prompt = lai.api.get_prompt(name="user intent")

messages = prompt.to_langchain_chat_prompt_template()
inp = messages.format_messages(
    support_ticket="The screen is cracked, there are scratches on the surface, and a component is missing."
)

# Initialize any model instance supported by LangChain
gpt_4o = init_chat_model(model_provider=prompt.provider, **prompt.settings)

# Literal AI LangChain Callback
cb = lai.langchain_callback()

# Invoke the model with input and callback configuration
completion = gpt_4o.invoke(inp, config=RunnableConfig(callbacks=[cb]))

# Print the completion result
print(completion)

lai.flush_and_stop()
```

