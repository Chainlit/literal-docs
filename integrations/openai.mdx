---
title: "OpenAI"
---

<Note>
  We only support the OpenAI python package starting at version `1.0.0`. We
  don't support the Assistant API yet.
</Note>

The OpenAI integration allows you to generate an LLM Step automatically for each call to the OpenAI API.


<CodeGroup>
```python Python
import os
from literalai import LiteralClient
from openai import OpenAI

"""
You need to call the `instrument_openai` method from the Literal client to
enable the integration. Call it before any OpenAI API call.
"""
openai_client = OpenAI()
literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
literal_client.instrument_openai()


@literal_client.thread()
def main():
    completion = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": "What is the country with the longest name?",
            }
        ],
    )

    print(completion.choices[0].message.content)


main()
literal_client.wait_until_queue_empty()
```

```typescript TypeScript
const stream = await openai.chat.completions.create({
  model: 'gpt-4',
  stream: true,
  messages: [{ role: 'user', content: 'Say this is a test' }]
});

// Create a child llm step
const childStep = step.childStep({
  name: 'gpt-4',
  type: 'llm',
  input: { content: 'Hello' }
});

// Instrument the openai response
await client.instrumentation.openai(childStep, stream);

// Send the child step
await childStep.send();
```
</CodeGroup>

