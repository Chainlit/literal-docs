---
title: Build and evaluate a RAG Chatbot
---

In this guide you'll learn how to build, improve and monitor a conversational application. We'll use [Chainlit](https://docs.chainlit.io/get-started/overview) in combination with Literal, to show the development flow of building, evaluating and improving the chatbot. The chatbot uses RAG to retrieve contexts for answer generation. 

## 1. Build a chatbot with Chainlit, using RAG

First, build a chatbot with Chainlit, using a RAG pipeline. Pinecone is used as vector database, but another vector store could also be used. Make sure to store your API keys of Literal, OpenAI and Pinecone in a `.env` file. Make sure you have data documents stored in this vector database.

```bash .env
LITERAL_API_KEY=
OPENAI_API_KEY=
PINECONE_API_KEY=
PINECONE_CLOUD=
PINECONE_REGION=
```

```bash requirements.txt
openai>=1.9.0
pinecone-client>=3.0.2
chainlit>=1.0.502
literalai>=0.0.501
ragas>=0.1.7
``` 

The Python code for this chatbot looks as follows. 
1. First, a Pinecone vector index is created. This vector database is used to store the documents and its embeddings of the data that you want your chatbot to use. For example, you can embed and store pages of a technical product documentation. 
2. This vector store is populated with some sample items. 
3. When a user query comes in, the query will be embedded.
4. The relevant documents from the vector store are retrieved and used as context to answer the user query. 
5. The LLM is used to generate an answer to the user query, using the retrieved documents.

Note how the different steps of the RAG pipeline are using Literal step decorators: `@cl.step(name="", type="")`. This is done in order to follow the steps in a thread in Literal. 

<CodeGroup>
```python Python
import chainlit as cl
import os
from dotenv import load_dotenv
from openai import AsyncOpenAI
from pinecone import Pinecone, ServerlessSpec
from literalai import LiteralClient
import asyncio

load_dotenv()

# Initiate the clients
literal_client = LiteralClient()
openai_client = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

pinecone_client = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))
pinecone_spec = ServerlessSpec(
    cloud=os.environ.get("PINECONE_CLOUD"), region=os.environ.get("PINECONE_REGION")
)

cl.instrument_openai()

# Create Pinecone vector index
def create_pinecone_index(name, client, spec):
    if name not in client.list_indexes().names():
        client.create_index(name, dimension=1536, metric="cosine", spec=spec)
    return pinecone_client.Index(name)


pinecone_index = create_pinecone_index(
    "literal-rag-index", pinecone_client, pinecone_spec
)

# Populate Pinecone index with documents
async def embed_and_store_documents(model="text-embedding-ada-002"):
    doc_1 = "Literal is a LLM observability, evaluation and prompt collaboration platform."
    doc_2 = "Literal has a UI and a Python and Typescript client."
    doc_1_embedding = await openai_client.embeddings.create(input=doc_1, model=model)
    doc_2_embedding = await openai_client.embeddings.create(input=doc_2, model=model)

    pinecone_index.upsert([
        (doc_1, doc_1_embedding.data[0].embedding),
        (doc_2, doc_2_embedding.data[0].embedding)
    ])

asyncio.run(embed_and_store_documents())

# Embedding the user query using a model from OpenAI
@cl.step(name="Embed", type="embedding")
async def embed(query, model="text-embedding-ada-002"):
    embedding = await openai_client.embeddings.create(input=query, model=model)

    return embedding.data[0].embedding

# Search and retrieve data items that are relevant to the user query from the Pinecone index, to create a context
@cl.step(name="Retrieve", type="retrieval")
async def retrieve(embedding):
    if pinecone_index == None:
        raise Exception("Pinecone index not initialized")
    response = pinecone_index.query(vector=embedding, top_k=5, include_metadata=True)

    return response

# query the LLM with the query and the context
@cl.step(name="LLM", type="llm")
async def llm(
    prompt,
    chat_model="gpt-4-turbo-preview",
):
    messages = cl.user_session.get("messages", [])
    messages.append(prompt)
    settings = {"temperature": 0, "stream": True, "model": chat_model}
    stream = await openai_client.chat.completions.create(messages=messages, **settings)
    message = cl.message.Message(content="")
    await message.send()

    async for part in stream:
        if token := part.choices[0].delta.content or "":
            await message.stream_token(token)

    await message.update()
    messages.append({"role": "assistant", "content": message.content})
    cl.user_session.set("messages", messages)
    return message.content


@cl.step(name="Query", type="run")
async def run(query):
    embedding = await embed(query)
    stored_embeddings = await retrieve(embedding)
    contexts = []
    prompt = literal_client.api.get_prompt(name="RAG prompt")

    if not prompt:
        raise Exception("Prompt not found")
    for match in stored_embeddings["matches"]:
        contexts.append(match["id"])

    completion = await llm(prompt.format({"context": contexts, "question": query})[-1])

    return completion


@cl.on_chat_start
async def on_chat_start():
    prompt = literal_client.api.get_prompt(name="RAG prompt")

    if not prompt:
        raise Exception("Prompt not found")
    cl.user_session.set(
        "messages",
        [prompt.format()[0]],
    )


@cl.on_message
async def main(message: cl.Message):
    await run(message.content)
```
</CodeGroup>


## 2. Log the chat conversations

If you are using Chainlit (as in the example above), logging of threads is automatically done. If you prefer using another method, like FastAPI, Flask or TypeScript, you can use the Literal SDK. For [Python](https://docs.getliteral.ai/python-client/abstractions/thread), use

<CodeGroup>
```python Python
with client.thread() as thread:
    # do something
```
</CodeGroup>

The threads or conversations will become visible in the Literal UI.

## 3. Run a few manual sample iterations and feedback

For initial testing purposes, you can manually run around 10 questions to your chatbot, in order to get familiar with the process and results. The results and steps that the chatbot took in order to come to an answer are visible in the Literal UI, under "Threads". In production, you want an extensive test, but it can be useful to get a first glance of the model.

Users can give feedback in the chatbot application to indicate how happy they are with the result. This gives you an indication on how well the model behaves.

As administrator, you can also give feedback on the chatbot's answers in the threads that already happened. 

## 4. Create a dataset and evaluate using RAGAS

In order to test the chatbot's model, you need to create a dataset. First, decide on the evaluation criteria that you want to test. [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/index.html) defines various metrics that can be tested in isolation, such as faithfulness, answer relevancy, context recall and answer correctness. In this tutorial, we are going to calculate **faithfulness** ans **answer correctness**. Faithfullness measures the factual consistency of the answer given the context. Faithfullness is measured by dividing *number of truthful claims that can be inferred from the given context* by the *total number of claims in the chatbot's generated answer*. Answer Correctness measures the accuracy of the generated answer compared to the ground truth. 

The example code shows how to create a dataset and data items in this dataset. The examples should match the evaluation criteria that we will test on: faithfulness and answer correctness. Thus, the facts in the answer should be correct given the context from the retriever, and the answer should be similar to the given ground truth. 

We will create a dataset with data that comes from the chatbot's conversations. The dataset will contain the user query, the chatbot's answer, and the relevant documents from the vector store that were used to generate the answer. The dataset will be used to evaluate the chatbot's model on faithfulness and answer correctness.

For the sake of this demo, we will write out the chatbot's answers here in code. In a production environment, for online evaluation, you can add the chatbot's answers to a created dataset via `dataset.add_step(step_id="<STEP_ID>")`. You can create a dataset via `dataset = sdk.api.create_dataset(name="Foo", description="A dataset to store samples.", metadata={ "isDemo": True })`

For evaluation, we use [RAGAS](https://docs.ragas.io/en/stable/index.html), a Python framework to evaluation RAG pipelines. Note that this evaluation is done *outside* of Literal.

<CodeGroup>
```python Python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_correctness
from datasets import Dataset

items = [
  {
    "input": "What is Literal?", 
    "expected_output": "Literal is a platform for LLM observability, evaluation and prompt collaboration.",
    "answer": "Literal is an observabilty solution",
    "context": ["Literal is a LLM observability, evaluation and prompt collaboration platform.", "Literal has a UI and a Python and Typescript client."]
  }, {
    "input": "What does Literal offer?", 
    "expected_output": "The platform has a UI and a Python and TypeScript client.",
    "answer": "Literal is an observabilty solution",
    "context": ["Literal is a LLM observability, evaluation and prompt collaboration platform.", "Literal has a UI and a Python and Typescript client."]
  }
]

test_dataset = {
    'question': [item["input"] for item in items],
    'answer': [item["answer"] for item in items],
    'contexts': [item["context"] for item in items],
    'ground_truth': [item["expected_output"] for item in items]
}

result = evaluate(
    Dataset.from_dict(test_dataset),
    metrics=[
        faithfulness, 
        answer_correctness],
)

df = result.to_pandas()
print(df.head())
```
</CodeGroup>

Note that for a serious evaluation you would want more than two examples to evaluate.

## 5. Analyze results

Finally, you can analyze the results of the evaluation. The results will be shown in the terminal.

<Frame caption="Evaluation result using RAGAS">
  <img src="/images/ragas-result.png" alt="Evaluation result using RAGAS"/>
</Frame>

You can see that both answers have the highest score for faithfulness, meaning that the chatbot's answers are factually consistent given the context. The answer correctness is also high for the first results, but low for the second result. This corresponds to human evaluation if we look at the examples we created. Based on this result, you can decide to improve the chatbot's model or prompt. 