---
title: RAG Chatbot Offline Evaluation
---

In this guide you'll learn how to build, improve and monitor a conversational application. We'll use [Chainlit](https://docs.chainlit.io/get-started/overview) in combination with Literal, to show the development flow of building, evaluating and improving the chatbot. The chatbot uses RAG to retrieve contexts for answer generation. 

## 1. Build a chatbot with Chainlit, using RAG

First, build a chatbot with Chainlit, using a RAG pipeline. Pinecone is used as vector database, but another vector store could also be used. Make sure to store your API keys of Literal, OpenAI and Pinecone in a `.env` file. Make sure you have data documents stored in this vector database.

```bash .env
LITERAL_API_KEY=
OPENAI_API_KEY=
PINECONE_API_KEY=
PINECONE_CLOUD=
PINECONE_REGION=
```

```bash requirements.txt
openai>=1.9.0
pinecone-client>=3.0.2
chainlit>=1.0.400
``` 

The Python code for this chatbot looks as follows. First, a Pinecone vector index is created. Then, note how the different steps of the RAG pipeline are using Literal step decorators: `@cl.step(name="", type="")`. This is done in order to follow the steps in a thread in Literal. 

<CodeGroup>
```python Python
import chainlit as cl
import os
from dotenv import load_dotenv
from openai import AsyncOpenAI
from pinecone import Pinecone, ServerlessSpec
from literalai import LiteralClient

load_dotenv()

# Initiate the clients
client = LiteralClient()
openai_client = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

pinecone_client = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))
pinecone_spec = ServerlessSpec(
    cloud=os.environ.get("PINECONE_CLOUD"), region=os.environ.get("PINECONE_REGION")
)

cl.instrument_openai()


def create_pinecone_index(name, client, spec):
    if name not in client.list_indexes().names():
        client.create_index(name, dimension=1536, metric="cosine", spec=spec)
    return pinecone_client.Index(name)


pinecone_index = create_pinecone_index(
    "literal-rag-index", pinecone_client, pinecone_spec
)


# Embedding the user query using a model from OpenAI
@cl.step(name="Embed", type="embedding")
async def embed(query, model="text-embedding-ada-002"):
    embedding = await openai_client.embeddings.create(input=query, model=model)

    return embedding.data[0].embedding


# Search and retrieve data items that are relevant to the user query from the Pinecone index, to create a context
@cl.step(name="Retrieve", type="retrieval")
async def retrieve(embedding):
    if pinecone_index == None:
        raise Exception("Pinecone index not initialized")
    response = pinecone_index.query(vector=embedding, top_k=5, include_metadata=True)

    return response


# query the LLM with the query and the context
@cl.step(name="LLM", type="llm")
async def llm(
    prompt,
    chat_model="gpt-4-turbo-preview",
):
    messages = cl.user_session.get("messages", [])
    messages.append(prompt)
    settings = {"temperature": 0, "stream": True, "model": chat_model}
    stream = await openai_client.chat.completions.create(messages=messages, **settings)
    message = cl.message.Message(content="")
    await message.send()

    async for part in stream:
        if token := part.choices[0].delta.content or "":
            await message.stream_token(token)

    await message.update()
    messages.append({"role": "assistant", "content": message.content})
    cl.user_session.set("messages", messages)
    return message.content


@cl.step(name="Query", type="run")
async def run(query):
    embedding = await embed(query)
    stored_embeddings = await retrieve(embedding)
    contexts = []
    prompt = await client.api.get_prompt(name="RAG prompt")

    if not prompt:
        raise Exception("Prompt not found")
    for match in stored_embeddings["matches"]:
        contexts.append(match["metadata"]["text"])

    completion = await llm(prompt.format({"context": contexts, "question": query})[-1])

    return completion


@cl.on_chat_start
async def on_chat_start():
    prompt = await client.api.get_prompt(name="RAG prompt")

    if not prompt:
        raise Exception("Prompt not found")
    cl.user_session.set(
        "messages",
        [prompt.format()[0]],
    )


@cl.on_message
async def main(message: cl.Message):
    await run(message.content)
```
</CodeGroup>


## 2. Log the chat conversations

If you are using Chainlit, logging of threads is automatically done. If you prefer using another method, like FastAPI, Flask or TypeScript, you can use the Literal SDK. For [Python](https://docs.getliteral.ai/python-client/abstractions/thread), use

<CodeGroup>
```python Python
with client.thread() as thread:
    # do something
```
</CodeGroup>

The threads or conversations will become visible in the Literal UI.

## 3. Run a few manual sample iterations and feedback

For initial testing purposes, you can manually run around 10 questions to your chatbot, in order to get familiar with the process and results. The results and steps that the chatbot took in order to come to an answer are visible in the Literal UI, under "Threads". In production, you want an extensive test, but it can be useful to get a first glance of the model.

Users can give feedback in the chatbot application to indicate how happy they are with the result. This gives you an indication on how well the model behaves.

As administrator, you can also give feedback on the chatbot's answers in the threads that already happened. 

## 4. Create an evaluation dataset

In order to test the chatbot's model, you need to create a dataset. First, decide on the evaluation criteria that you want to test. [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/index.h) defines various metrics that can be tested in isolation, such as faithfulness, answer relevancy, context recall and answer correctness. In this tutorial, we are going to calculate **faithfulness**, which measures the factual consistency of the answer given the context. Faithfullness is measured by dividing *number of truthful claims that can be inferred from the given context* by the *total number of claims in the chatbot's generated answer*.

Create a dataset: 

<CodeGroup>
```python Python
from literalai import LiteralClient

client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

dataset = await client.api.create_dataset(
  name="EvaluationDataset", description="A dataset to store samples for model evaluation.", metadata={"isDemo": True}
)
```
</CodeGroup>

Then, create data items in this dataset. The examples should match the evaluation criteria that we will test on: faithfulness. Thus, the facts in the answer should be correct given the context from the retriever. 

<CodeGroup>
```python Python
# add inputs, questions that a user might ask to the chatbot. Add more if desired.
inputs = [
    "What is Literal?",
    "What are the key features of Literal?",
    "What is a thread?",
    "What is a step?"
]

# add outputs that are the desired answers from the chatbot to the users' questions
outputs = [
    "Literal is an observability solution.",
    "The key features of literal are observability, creating datasets, doing online evaluation and perform prompt collaboration.",
    "In Literal, a thread represents a group of interactions between a user and an AI app.",
    "In Literal, a step is a fundamental building block within a thread that represents a single unit of interaction or operation."
]

# add these inputs and outputs to the previously created dataset
dataset_items = await dataset.create_dataset_items(
  dataset_id = dataset.id,
  inputs = [{"content": i} for i in inputs],
  expected_outputs = [{"content": o} for o in outputs]
)

# ???????????? TO DO 
step_item = await dataset.add_step_to_dataset(dataset.id, step.id)
```
</CodeGroup>

Note that for a serious evaluation you would want more than four examples to evaluate.

## 5. Predict and evaluate

The next step is to run the evaluation. We configure the metric here to "faithfulness".

<CodeGroup>
```python Python
# configure the evaluation
evaluation_config = {
    "metrics": ["faithfulness"]
}

# do the evaluation
await client.evaluate(
  dataset_id = dataset.id,
  config = evaluation_config
)
```
</CodeGroup>


## 6. Analyze results

[TO DO: add screenshots of evaluation results in Literal]