---
title: Build and evaluate a RAG Chatbot with Chainlit, OpenAI, Pinecone and Literal
---

In this guide you'll learn how to build, improve and monitor a conversational application. We'll use [Chainlit](https://docs.chainlit.io/get-started/overview) in combination with Literal, to show the development flow of building, evaluating and improving the chatbot. The chatbot uses RAG to retrieve contexts for answer generation. 

## 1. Build a chatbot with Chainlit, using RAG

First, build a chatbot with Chainlit, using a RAG pipeline. Pinecone is used as vector database, but another vector store could also be used. Make sure to store your API keys of Literal, OpenAI and Pinecone in a `.env` file. Make sure you have data documents stored in this vector database.

```bash .env
LITERAL_API_KEY=
OPENAI_API_KEY=
PINECONE_API_KEY=
PINECONE_CLOUD=
PINECONE_REGION=
```

```bash requirements.txt
openai>=1.9.0
pinecone-client>=3.0.2
chainlit>=1.0.502
literalai>=0.0.501
ragas>=0.1.7
``` 

You can browse the full code for this chatbot at [Documentation - RAG application](https://github.com/Chainlit/Documentation-RAG-application).

Beyond the necessary imports, here is how the Python code for this chatbot is structured. 
1. First, a Pinecone vector index is created. This vector database is used to store the documents and its embeddings of the data that you want your chatbot to use. For example, you can embed and store pages of a technical product documentation.  
Check out the "Embed the documentation" section to populate your own Pinecone index with your own documents.

<CodeGroup>
```python Set up Pinecone
pinecone_client = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))
pinecone_spec = ServerlessSpec(
    cloud=os.environ.get("PINECONE_CLOUD"),
    region=os.environ.get("PINECONE_REGION"),
)

def create_pinecone_index(name, client, spec):
    """
    Create a Pinecone index if it does not already exist.
    """
    if name not in client.list_indexes().names():
        client.create_index(name, dimension=1536, metric="cosine", spec=spec)
    return pinecone_client.Index(name)

pinecone_index = create_pinecone_index(
    "literal-rag-index", pinecone_client, pinecone_spec
)
```
</CodeGroup>


2. When a user question comes in, we use a prompt defined in file - or fetched from Literal,
and we trigger our RAG agent logic.

<CodeGroup>
```python Setup chat prompt template
prompt_path = os.path.join(os.getcwd(), "./app/prompts/rag.json")

@cl.on_chat_start
async def on_chat_start():
    """
    Send a welcome message and set up the initial user session on chat start.
    """
    await cl.Message(
        content="Welcome, please ask me anything about the Literal documentation !"
    ).send()

    # We load the RAG prompt in Literal to track prompt iteration and
    # enable LLM replays from Literal AI.
    with open(prompt_path, "r") as f:
        rag_prompt = json.load(f)

        prompt = await client.api.get_or_create_prompt(
            name=rag_prompt["name"],
            template_messages=rag_prompt["template_messages"],
            settings=rag_prompt["settings"],
            tools=rag_prompt["tools"],
        )

    cl.user_session.set("messages", prompt.format_messages())
    cl.user_session.set("settings", prompt.settings)
    cl.user_session.set("tools", prompt.tools)


@cl.on_message
async def main(message: cl.Message):
    """
    Main message handler for incoming user messages.
    """
    # Hack to have agent logic part of Chatbot answer.
    answer_message = cl.Message(content="")
    await answer_message.send()
    cl.user_session.set("answer_message", answer_message)

    await rag_agent(message.content)


@cl.step(name="RAG Agent", type="run")
async def rag_agent(question) -> str:
    """
    Coordinate the RAG agent flow to generate a response based on the user's question.
    """
    # Step 1 - Call LLM with tool: plan to use tool or give message.
    message = await llm_tool(question)

    # Potentially several calls to retrieve context.
    if not message.tool_calls:
        return message.content

    # Step 2 - Run the tool calls.
    tool_results = await run_multiple(message.tool_calls)

    # Step 3 - Call LLM to answer based on contexts (streamed).
    return await llm_answer(tool_results)
```
</CodeGroup>


3. Perform the LLM call with mention of the Pinecone tool.

<CodeGroup>

```python Call LLM with tool option
async def llm_tool(question):
    """
    Generate a response from the LLM based on the user's question.
    """
    messages = cl.user_session.get("messages", []) or []
    messages.append({"role": "user", "content": question})

    settings = cl.user_session.get("settings", {}) or {}
    settings["tools"] = cl.user_session.get("tools")
    settings["tool_choice"] = "auto"

    response = await openai_client.chat.completions.create(
        messages=messages,
        **settings,
    )

    response_message = response.choices[0].message
    messages.append(response_message)
    return response_message
```
</CodeGroup>

4. The relevant documents from the vector store are retrieved and used as context to answer the user query. 

<CodeGroup>
```python Run tool calls

@cl.step(name="Embed", type="embedding")
async def embed(question, model="text-embedding-ada-002"):
    """
    Embed a question using the specified model and return the embedding.
    """
    embedding = await openai_client.embeddings.create(input=question, model=model)

    return embedding.data[0].embedding


@cl.step(name="Retrieve", type="retrieval")
async def retrieve(embedding, top_k):
    """
    Retrieve top_k closest vectors from the Pinecone index using the provided embedding.
    """
    if pinecone_index == None:
        raise Exception("Pinecone index not initialized")
    response = pinecone_index.query(
        vector=embedding, top_k=top_k, include_metadata=True
    )
    return response.to_dict()


@cl.step(name="Retrieval", type="tool")
async def get_relevant_documentation_chunks(question, top_k=5):
    """
    Retrieve relevant documentation chunks based on the question embedding.
    """
    embedding = await embed(question)

    retrieved_chunks = await retrieve(embedding, top_k)

    contexts = [match["metadata"]["text"] for match in retrieved_chunks["matches"]]

    return "\n".join(contexts)

async def run_multiple(tool_calls):
    """
    Execute multiple tool calls asynchronously.
    """
    available_tools = {
        "get_relevant_documentation_chunks": get_relevant_documentation_chunks
    }

    async def run_single(tool_call):
        function_name = tool_call.function.name
        function_to_call = available_tools[function_name]
        function_args = json.loads(tool_call.function.arguments)

        # TODO: Parametrize the arguments depending on tool.
        function_response = await function_to_call(
            question=function_args.get("question"),
            top_k=function_args.get("top_k"),
        )
        return {
            "tool_call_id": tool_call.id,
            "role": "tool",
            "name": function_name,
            "content": function_response,
        }

    # Run tool calls in parallel.
    tool_results = await asyncio.gather(
        *(run_single(tool_call) for tool_call in tool_calls)
    )
    return tool_results
```

</CodeGroup>

5. The LLM is used to generate an answer to the user query, using the retrieved documents.

<CodeGroup>
```python Generate answer
async def llm_answer(tool_results):
    """
    Generate an answer from the LLM based on the results of tool calls.
    """
    messages = cl.user_session.get("messages", []) or []
    messages.extend(tool_results)

    settings = cl.user_session.get("settings", {}) or {}
    settings["stream"] = True

    stream = await openai_client.chat.completions.create(
        messages=messages,
        **settings,
    )

    answer_message = cl.user_session.get("answer_message")

    async for part in stream:
        if token := part.choices[0].delta.content or "":
            await answer_message.stream_token(token)
    await answer_message.update()

    messages.append({"role": "assistant", "content": answer_message.content})
    return answer_message.content
```
</CodeGroup>

Note how the different steps of the RAG pipeline are using Literal step decorators: `@cl.step(name="", type="")`. This is done in order to follow the steps in a thread in Literal. 


## 2. Log the chat conversations

If you are using Chainlit (as in the example above), logging of threads is automatically done. If you prefer using another method, like FastAPI, Flask or TypeScript, you can use the Literal SDK. For [Python](https://docs.getliteral.ai/python-client/abstractions/thread), use

<CodeGroup>
```python Python
with client.thread() as thread:
    # do something
```
</CodeGroup>

The threads or conversations will become visible in the Literal UI.

## 3. Run a few manual sample iterations and feedback

For initial testing purposes, you can manually run a few questions to your chatbot, in order to get familiar with the process and results. The results and steps that the chatbot took in order to come to an answer are visible in the Literal UI, under "Threads". For a real application, you want an extensive test, but it can be useful to get a first glance of the model.

Users can give feedback in the chatbot application to indicate how happy they are with the result. This gives you an indication on how well the model behaves.

As administrator, you can also give feedback on the chatbot's answers in the threads that already happened. 

## 4. Create a Dataset

In order to test the chatbot's model, you need to create a dataset. First, decide on the evaluation criteria that you want to test. [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/index.html) defines various metrics that can be tested in isolation, such as faithfulness, answer relevancy, context recall and answer correctness. In this tutorial, we are going to calculate **faithfulness** ans **answer correctness**. Faithfullness measures the factual consistency of the answer given the context. Faithfullness is measured by dividing *number of truthful claims that can be inferred from the given context* by the *total number of claims in the chatbot's generated answer*. Answer Correctness measures the accuracy of the generated answer compared to the ground truth. 

The dataset for this experiment will consist of examples that we add by hand. The examples have an `input` and an `expected_output`. The `input` is the user query, and the `expected_output` is the expected answer from the chatbot. 

<CodeGroup>
```python Python
# Create Dataset in Literal
dataset = literal_client.api.create_dataset(
    name="TestDataset", description="A dataset to store samples.", metadata={ "isDemo": True }
)

# The items with ground truth to add to the dataset
items = [
  {
    "input": "What is Literal?", 
    "expected_output": "Literal is a platform for LLM observability, evaluation and prompt collaboration.",
  }, {
    "input": "What does Literal offer?", 
    "expected_output": "The platform has a UI and a Python and TypeScript client.",
  }
]

# Add items to Dataset in Literal
for item in items:
    dataset.create_item(
      input={ "content": item["input"] },
      expected_output={ "content": item["expected_output"] }
    )
```
</CodeGroup>

Note that for a serious evaluation you would want more than two examples to evaluate.

## 5. Run chatbot on dataset items

In order to evaluate the chatbot, we need to run the test cases that we saved in the dataset using the LLM model of the chatbot. We will add the chatbot's answers and contexts to the item list. Run the input questions that you put in the test dataset on the chatbot. In this example, Chainlit is used. You can open the Chainlit chatbot and ask your questions there, in two separate chats. These threads will be visible in the Literal UI, under "Threads". 

<Frame caption="Test Threads">
  <img src="/images/rag-chatbot-threads.png" alt="Test Threads"/>
</Frame>

<Frame caption="Test Thread">
  <img src="/images/rag-chatbot-thread.png" alt="Test Thread"/>
</Frame>

Next, you want to add the chatbot's answers and contexts to the list of test items that we created in the previous step. In the next step we can use this list to evaluate using Ragas. Make sure to use the `Step ID` from the `LLM` step in the Thread, and `Step ID` from the `Retrieve` step in the Thread.


<CodeGroup>
```python Python
import ast 
# Replace with your step IDs
llm_steps = ["b5f69f7e-a9d2-4a6f-8b1a-61e8a2c0a374", "b165f8c2-ee08-44ab-9b3a-ae05a669ccaa"]
context_steps = ["3c848131-bb52-4f38-85c1-afdc1422e748", "2d96877d-4beb-4337-978b-f8c19daf3527"]

# Add answers to the item list
for idx, llm_step in enumerate(llm_steps):
    llm_step_data = literal_client.api.get_step(llm_step)
    items[idx]["answer"] = llm_step_data.output["content"]

    contexts = []
    contexts_data = literal_client.api.get_step(context_steps[idx])
    contexts_data_json = ast.literal_eval(contexts_data.output["content"])["matches"]
    for context in contexts_data_json:
        contexts.append(context["id"])
    items[idx]["context"] = contexts
```
</CodeGroup>

## 6. Evaluate items 

Next, we can evaluate the two generated answers on faithfulness and answer correctness. For evaluation, we use [Ragas](https://docs.ragas.io/en/stable/index.html), a Python framework to evaluation RAG pipelines. Note that this evaluation is done *outside* of Literal (offline).


<CodeGroup>
```python Python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_correctness
from datasets import Dataset

test_dataset = {
    'question': [item["input"] for item in items],
    'answer': [item["answer"] for item in items],
    'contexts': [item["context"] for item in items],
    'ground_truth': [item["expected_output"] for item in items]
}

result = evaluate(
    Dataset.from_dict(test_dataset),
    metrics=[
        faithfulness, 
        answer_correctness],
)

df = result.to_pandas()
print(df.head())
```
</CodeGroup>

## 5. Analyze results

Finally, you can analyze the results of the evaluation. The results will be shown in the terminal.

<Frame caption="Evaluation result using RAGAS">
  <img src="/images/ragas-result.png" alt="Evaluation result using RAGAS"/>
</Frame>

You can see that both answers have the highest score for faithfulness, meaning that the chatbot's answers are factually consistent given the context. The answer correctness is also high for the first results, but low for the second result. This corresponds to human evaluation if we look at the examples we created. Based on this result, you can decide to improve the chatbot's model or prompt. 

You can also see the experiment results in Literal, if you upload the experiment. 

<CodeGroup>
```python Python
experiment = dataset.create_experiment(
    name="Test Experiment",
    prompt_id= literal_client.api.get_prompt(name="RAG prompt").id,
    params=[{ "type": faithfulness.name, "top_k": 1 }, { "type": answer_correctness.name, "top_k": 1 }]
)

# Log each experiment result.
for index, row in df.iterrows():
    scores = [{ 
        "name": faithfulness.name,
        "type": "AI",
        "value": row[faithfulness.name]
    }, { 
        "name": answer_correctness.name,
        "type": "AI",
        "value": row[answer_correctness.name]
    }]

    experiment_item = {
        "datasetItemId": dataset.items[index].id,
        "scores": scores,
        "input": { "question": row["question"] },
        "output": { "contexts": row["contexts"].tolist() }
    }
    
    experiment.log(experiment_item)
```
</CodeGroup>

<Frame caption="Evaluation result using RAGAS in Literal">
  <img src="/images/rag-chatbot-experiment.png" alt="Evaluation result using RAGAS in Literal"/>
</Frame>