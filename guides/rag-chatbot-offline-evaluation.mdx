---
title: Build and evaluate a RAG Chatbot
---

In this guide you'll learn how to build, improve and monitor a conversational application. We'll use [Chainlit](https://docs.chainlit.io/get-started/overview) in combination with Literal, to show the development flow of building, evaluating and improving the chatbot. The chatbot uses RAG to retrieve contexts for answer generation. 

## 1. Build a chatbot with Chainlit, using RAG

First, build a chatbot with Chainlit, using a RAG pipeline. Pinecone is used as vector database, but another vector store could also be used. Make sure to store your API keys of Literal, OpenAI and Pinecone in a `.env` file. Make sure you have data documents stored in this vector database.

```bash .env
LITERAL_API_KEY=
OPENAI_API_KEY=
PINECONE_API_KEY=
PINECONE_CLOUD=
PINECONE_REGION=
```

```bash requirements.txt
openai>=1.9.0
pinecone-client>=3.0.2
chainlit>=1.0.502
literalai>=0.0.501
ragas>=0.1.7
``` 

The Python code for this chatbot looks as follows. 
1. First, a Pinecone vector index is created. This vector database is used to store the documents and its embeddings of the data that you want your chatbot to use. For example, you can embed and store pages of a technical product documentation. 
2. This vector store is populated with some sample items. 
3. When a user query comes in, the query will be embedded.
4. The relevant documents from the vector store are retrieved and used as context to answer the user query. 
5. The LLM is used to generate an answer to the user query, using the retrieved documents.

Note how the different steps of the RAG pipeline are using Literal step decorators: `@cl.step(name="", type="")`. This is done in order to follow the steps in a thread in Literal. 

Run the following code, named `chatbot.py`, with `chainlit run chatbot.py`.

<CodeGroup>
```python Python
import chainlit as cl
import os
from dotenv import load_dotenv
from openai import AsyncOpenAI
from pinecone import Pinecone, ServerlessSpec
from literalai import LiteralClient
import asyncio

load_dotenv()

# Initiate the clients
literal_client = LiteralClient()
openai_client = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

pinecone_client = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))
pinecone_spec = ServerlessSpec(
    cloud=os.environ.get("PINECONE_CLOUD"), region=os.environ.get("PINECONE_REGION")
)

cl.instrument_openai()

# Create Pinecone vector index
def create_pinecone_index(name, client, spec):
    if name not in client.list_indexes().names():
        client.create_index(name, dimension=1536, metric="cosine", spec=spec)
    return pinecone_client.Index(name)


pinecone_index = create_pinecone_index(
    "literal-rag-index", pinecone_client, pinecone_spec
)

# Populate Pinecone index with documents
async def embed_and_store_documents(model="text-embedding-ada-002"):
    doc_1 = "Literal is a LLM observability, evaluation and prompt collaboration platform."
    doc_2 = "Literal has a UI and a Python and Typescript client."
    doc_1_embedding = await openai_client.embeddings.create(input=doc_1, model=model)
    doc_2_embedding = await openai_client.embeddings.create(input=doc_2, model=model)

    pinecone_index.upsert([
        (doc_1, doc_1_embedding.data[0].embedding),
        (doc_2, doc_2_embedding.data[0].embedding)
    ])

asyncio.run(embed_and_store_documents())

# Embedding the user query using a model from OpenAI
@cl.step(name="Embed", type="embedding")
async def embed(query, model="text-embedding-ada-002"):
    embedding = await openai_client.embeddings.create(input=query, model=model)

    return embedding.data[0].embedding

# Search and retrieve data items that are relevant to the user query from the Pinecone index, to create a context
@cl.step(name="Retrieve", type="retrieval")
async def retrieve(embedding):
    if pinecone_index == None:
        raise Exception("Pinecone index not initialized")
    response = pinecone_index.query(vector=embedding, top_k=5, include_metadata=True)

    return response

# query the LLM with the query and the context
@cl.step(name="LLM", type="llm")
async def llm(
    prompt,
    chat_model="gpt-4-turbo-preview",
):
    messages = cl.user_session.get("messages", [])
    messages.append(prompt)
    settings = {"temperature": 0, "stream": True, "model": chat_model}
    stream = await openai_client.chat.completions.create(messages=messages, **settings)
    message = cl.message.Message(content="")
    await message.send()

    async for part in stream:
        if token := part.choices[0].delta.content or "":
            await message.stream_token(token)

    await message.update()
    messages.append({"role": "assistant", "content": message.content})
    cl.user_session.set("messages", messages)
    return message.content


@cl.step(name="Query", type="run")
async def run(query):
    embedding = await embed(query)
    stored_embeddings = await retrieve(embedding)
    contexts = []
    prompt = literal_client.api.get_prompt(name="RAG prompt")

    if not prompt:
        raise Exception("Prompt not found")
    for match in stored_embeddings["matches"]:
        contexts.append(match["id"])

    completion = await llm(prompt.format({"context": contexts, "question": query})[-1])

    return completion


@cl.on_chat_start
async def on_chat_start():
    prompt = literal_client.api.get_prompt(name="RAG prompt")

    if not prompt:
        raise Exception("Prompt not found")
    cl.user_session.set(
        "messages",
        [prompt.format()[0]],
    )


@cl.on_message
async def main(message: cl.Message):
    await run(message.content)
```
</CodeGroup>


## 2. Log the chat conversations

If you are using Chainlit (as in the example above), logging of threads is automatically done. If you prefer using another method, like FastAPI, Flask or TypeScript, you can use the Literal SDK. For [Python](https://docs.getliteral.ai/python-client/abstractions/thread), use

<CodeGroup>
```python Python
with client.thread() as thread:
    # do something
```
</CodeGroup>

The threads or conversations will become visible in the Literal UI.

## 3. Run a few manual sample iterations and feedback

For initial testing purposes, you can manually run a few questions to your chatbot, in order to get familiar with the process and results. The results and steps that the chatbot took in order to come to an answer are visible in the Literal UI, under "Threads". For a real application, you want an extensive test, but it can be useful to get a first glance of the model.

Users can give feedback in the chatbot application to indicate how happy they are with the result. This gives you an indication on how well the model behaves.

As administrator, you can also give feedback on the chatbot's answers in the threads that already happened. 

## 4. Create a Dataset

In order to test the chatbot's model, you need to create a dataset. First, decide on the evaluation criteria that you want to test. [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/index.html) defines various metrics that can be tested in isolation, such as faithfulness, answer relevancy, context recall and answer correctness. In this tutorial, we are going to calculate **faithfulness** ans **answer correctness**. Faithfullness measures the factual consistency of the answer given the context. Faithfullness is measured by dividing *number of truthful claims that can be inferred from the given context* by the *total number of claims in the chatbot's generated answer*. Answer Correctness measures the accuracy of the generated answer compared to the ground truth. 

The dataset for this experiment will consist of examples that we add by hand. The examples have an `input` and an `expected_output`. The `input` is the user query, and the `expected_output` is the expected answer from the chatbot. 

<CodeGroup>
```python Python
# Create Dataset in Literal
dataset = literal_client.api.create_dataset(
    name="TestDataset", description="A dataset to store samples.", metadata={ "isDemo": True }
)

# The items with ground truth to add to the dataset
items = [
  {
    "input": "What is Literal?", 
    "expected_output": "Literal is a platform for LLM observability, evaluation and prompt collaboration.",
  }, {
    "input": "What does Literal offer?", 
    "expected_output": "The platform has a UI and a Python and TypeScript client.",
  }
]

# Add items to Dataset in Literal
for item in items:
    dataset.create_item(
      input={ "content": item["input"] },
      expected_output={ "content": item["expected_output"] }
    )
```
</CodeGroup>

Note that for a serious evaluation you would want more than two examples to evaluate.

## 5. Run chatbot on dataset items

In order to evaluate the chatbot, we need to run the test cases that we saved in the dataset using the LLM model of the chatbot. We will add the chatbot's answers and contexts to the item list. Run the input questions that you put in the test dataset on the chatbot. In this example, Chainlit is used. You can open the Chainlit chatbot and ask your questions there, in two separate chats. These threads will be visible in the Literal UI, under "Threads". 

<Frame caption="Test Threads">
  <img src="/images/rag-chatbot-threads.png" alt="Test Threads"/>
</Frame>

<Frame caption="Test Thread">
  <img src="/images/rag-chatbot-thread.png" alt="Test Thread"/>
</Frame>

Next, you want to add the chatbot's answers and contexts to the list of test items that we created in the previous step. In the next step we can use this list to evaluate using RAGAS. Make sure to use the `Step ID` from the `LLM` step in the Thread, and `Step ID` from the `Retrieve` step in the Thread.


<CodeGroup>
```python Python
import ast 
# Replace with your step IDs
llm_steps = ["b5f69f7e-a9d2-4a6f-8b1a-61e8a2c0a374", "b165f8c2-ee08-44ab-9b3a-ae05a669ccaa"]
context_steps = ["3c848131-bb52-4f38-85c1-afdc1422e748", "2d96877d-4beb-4337-978b-f8c19daf3527"]

# Add answers to the item list
for idx, llm_step in enumerate(llm_steps):
    llm_step_data = literal_client.api.get_step(llm_step)
    items[idx]["answer"] = llm_step_data.output["content"]

    contexts = []
    contexts_data = literal_client.api.get_step(context_steps[idx])
    contexts_data_json = ast.literal_eval(contexts_data.output["content"])["matches"]
    for context in contexts_data_json:
        contexts.append(context["id"])
    items[idx]["context"] = contexts
```
</CodeGroup>

## 6. Evaluate items 

Next, we can evaluate the two generated answers on faithfulness and answer correctness. For evaluation, we use [RAGAS](https://docs.ragas.io/en/stable/index.html), a Python framework to evaluation RAG pipelines. Note that this evaluation is done *outside* of Literal.


<CodeGroup>
```python Python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_correctness
from datasets import Dataset

test_dataset = {
    'question': [item["input"] for item in items],
    'answer': [item["answer"] for item in items],
    'contexts': [item["context"] for item in items],
    'ground_truth': [item["expected_output"] for item in items]
}

result = evaluate(
    Dataset.from_dict(test_dataset),
    metrics=[
        faithfulness, 
        answer_correctness],
)

df = result.to_pandas()
print(df.head())
```
</CodeGroup>

## 5. Analyze results

Finally, you can analyze the results of the evaluation. The results will be shown in the terminal.

<Frame caption="Evaluation result using RAGAS">
  <img src="/images/ragas-result.png" alt="Evaluation result using RAGAS"/>
</Frame>

You can see that both answers have the highest score for faithfulness, meaning that the chatbot's answers are factually consistent given the context. The answer correctness is also high for the first results, but low for the second result. This corresponds to human evaluation if we look at the examples we created. Based on this result, you can decide to improve the chatbot's model or prompt. 

You can also see the experiment results in Literal, if you upload the experiment. 

<CodeGroup>
```python Python
experiment = dataset.create_experiment(
    name="Test Experiment",
    prompt_id= literal_client.api.get_prompt(name="RAG prompt").id,
    params=[{ "type": faithfulness.name, "top_k": 1 }, { "type": answer_correctness.name, "top_k": 1 }]
)

# Log each experiment result.
for index, row in df.iterrows():
    scores = [{ 
        "name": faithfulness.name,
        "type": "AI",
        "value": row[faithfulness.name]
    }, { 
        "name": answer_correctness.name,
        "type": "AI",
        "value": row[answer_correctness.name]
    }]

    experiment_item = {
        "datasetItemId": dataset.items[index].id,
        "scores": scores,
        "input": { "question": row["question"] },
        "output": { "contexts": row["contexts"].tolist() }
    }
    
    experiment.log(experiment_item)
```
</CodeGroup>

<Frame caption="Evaluation result using RAGAS in Literal">
  <img src="/images/rag-chatbot-experiment.png" alt="Evaluation result using RAGAS in Literal"/>
</Frame>