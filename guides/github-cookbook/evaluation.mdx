---
title: Evaluation
---

You can find the code here: https://github.com/Chainlit/literal-cookbook.


- [Prompt Iteration with **Promptfoo**](https://github.com/Chainlit/literal-cookbook/tree/main/typescript/prompt-iteration-promptfoo):
Our cookbook shows how a developer can leverage the `promptfoo` library in TypeScript to
compute similarity scores and persist the results of their experiments directly on Literal AI. 
- [Context Relevancy with **Ragas**](https://github.com/Chainlit/literal-cookbook/blob/main/python/context-relevancy-ragas/Context%20relevancy%20with%20Ragas.ipynb): A Python notebook of experiments on an example RAG
application to score context relevancy.
- [Evaluating User Satisfaction & Customer Support Conversations](https://github.com/Chainlit/literal-cookbook/blob/main/python/evaluate-user-satisfaction/Evaluate%20user%20satisfaction.ipynb): This Python notebook shows how to retrieve your Customer Support Conversations from Literal and evaluate user satisfaction on this conversational data.
- [Client-side A/B testing two prompts](https://github.com/Chainlit/literal-cookbook/blob/main/python/ab-testing-client-side/ab-testing-client-side.ipynb): Build two prompts, randomly assign to new conversations and A/B test on a metric.
- [Evaluate Agent Runs with Tools](https://github.com/Chainlit/literal-cookbook/blob/main/python/evaluate-agent-runs/Agent%20runs%20-%20Evaluation.ipynb): Build a simple agent which can use two tools. Monitor and evaluate the tool usage.