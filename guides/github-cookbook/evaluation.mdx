---
title: Evaluation
---

You can find the code here: https://github.com/Chainlit/literal-cookbook/tree/main/evaluation


- [Prompt Iteration with **Promptfoo**](https://github.com/Chainlit/literal-cookbook/tree/main/evaluation/typescript/prompt-iteration-promptfoo):
Our cookbook shows how a developer can leverage the `promptfoo` library in TypeScript to
compute similarity scores and persist the results of their experiments directly on Literal AI.
- [Context Relevancy with **Ragas**](https://github.com/Chainlit/literal-cookbook/tree/main/evaluation/python): our notebook experiments on an example RAG
application to score context relevancy.

