---
title: Evaluation
---

You can find the code here: https://github.com/Chainlit/literal-cookbook/tree/main/evaluation.


- [Prompt Iteration with **Promptfoo**](https://github.com/Chainlit/literal-cookbook/tree/main/evaluation/typescript/prompt-iteration-promptfoo):
Our cookbook shows how a developer can leverage the `promptfoo` library in TypeScript to
compute similarity scores and persist the results of their experiments directly on Literal AI. 
- [Context Relevancy with **Ragas**](https://github.com/Chainlit/literal-cookbook/tree/main/evaluation/python): A Python notebook of experiments on an example RAG
application to score context relevancy.
- [Evaluating User Satisfaction & Customer Support Conversations](https://github.com/Chainlit/literal-cookbook/blob/main/evaluation/python/Evaluate%20user%20satisfaction.ipynb): This Python notebook shows how to retrieve your Customer Support Conversations from Literal and evaluate user satisfaction on this conversational data.