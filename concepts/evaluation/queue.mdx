---
title: "Annotation Queues"
---

The Literal AI platform helps you organise and streamline the human evaluation process by easily setting-up annotation queues.
Each queue let you add data for review and provides an accessible and efficient ui for domain experts to score, tag and add to datasets crucial insights.

## Creating an annotation queue

You can access your queues on the "Annotation Queues" page, and create them by pressing the "+" button in the top right corner of the table.

<img src="/images/create-annotation-queue.png" alt="Create annotation queue image" />

This will open a modal form, that you can fill with a **name** to identify the annotation queue and a **description** to convey the queue meaning.

<img src="/images/create-annotation-queue-form.png" alt="Create annotation queue image form" />

<Note>Each new project comes with a "Default" annotation queue.</Note>

## Populating a queue with data

With your queues created you can manually add items to review by accessing your run/generation logs, selecting them and clicking "Add to Annotation Queue".

<img src="/images/add-generations-queue.png" alt="Add generations to the queue" />

To more precisely pick the relevant data you are able to select specific steps.

<img src="/images/add-step-queue.png" alt="Add steps to the queue" />

## Review queues

On the "Annotation Queues" page, a reviewer can open a specific queue and start reviewing added items. They can provide you with human feedback by scoring and tagging each item.

<img src="/images/annotation-item-view.png" alt="The annotation item page" />

A reviewer can also add a specific step data to an existing dataset.

Finally, once the work is done, the item can be marked as reviewed.

## More granular control

To keep track of what is happening in the queues, we also provide for super users a proper list of the items in the queues.

<img src="/images/annotation-item-list.png" alt="The annotation item list page" />


