---
title: "Online Evaluation"
description: "Evaluate LLM generations on the fly!"
---

## Introduction

Online evaluation lets you create and delete **Rules** which automatically evaluate your LLM application.  

The current scope for Online Evaluation is focused on LLM Generations and allows you to classify LLM generations into categories of a [Score Template](score#from-the-application). 

To get started, browse to the **Online Evaluation** menu in the navigation side bar to open the **Online Evaluation** page where you can manage **Rules** and **Tasks**.

<Frame caption="Online Evaluation">
  <img src="/images/online-evaluation-page.png" alt="Online Evaluation page" />
</Frame>

### Rules

To create a **Rule**, fill in the form template shown below. 
The specified provider will be used as an LLM judge to score a percentage of your ingested LLM generations by classifying them according to the specified score template.  

The evaluations take place upon ingestion of Generation and are run against the LLM provider of your choice. Make sure to create and link to [Shared Credentials](../prompt/overview#shared-credentials) and to carefully set the sample
rate to a reasonable value to avoid incurring high costs from your LLM provider.

<Frame caption="Create Rule Dialog">
  <img src="/images/create-rule-dialog.png" alt="Create Rule Dialog" />
</Frame>

<Warning>The Score Template categories must be explicit as the prompt used to evaluate generations relies solely on the categories names, not on the Score Template name.</Warning>

Here are two examples of correct vs. incorrect Score Template:

            Correct Score Template          |            Incorrect Score Template
:------------------------------------------:|:------------------------------------------:
![](images/correct-rule-score-template.png) |  ![](images/incorrect-rule-score-template.png)

### Tasks

When your LLM-based application triggers a call to an LLM, a **Generation** object is ingested and a **Rule** may trigger to evaluate that Generation, based on its sample rate.  

Each time a **Rule** is triggered to evaluate a Generation, the LLM judge is triggered in an asynchronous **Task**.  

You can review the triggered **Rules** and the Generations they ran against via **Tasks** and can directly jump to the evaluated Generation to view its score!

<Frame caption="Evaluation Tasks">
  <img src="/images/evaluation-tasks.png" alt="Evaluation Tasks" />
</Frame>

The LLM-generated AI score will show the category value along with a reason behind this evaluation: 
<Frame caption="AI score generated via Rules">
  <img src="/images/ai-score-via-rule.png" alt="AI score generated via Rules" />
</Frame>

With **Rules** properly set up, you can easily perform semantic sentiment analysis on your LLM generations, with categories such as Positive/Neutral/Negative. 
From the **Generations** table, filters let you quickly have access to the negative generations for review and action.



