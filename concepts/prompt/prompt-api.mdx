---
title: Prompt API
---

Via the Python and TypeScript SDK, you can acces the prompt API. With the API you can get the prompt to use in your code and add variable values in a conversation flow. You cannot draft and save prompt templates via the API, this should be done via the Literal AI Prompt Playground.

### Get a Prompt

Getting a prompt is the first step to use it in your code. You can get a prompt by its name. Prompt names are their IDs (hence you cannot change the name).

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
# This will fetch the champion version, you can also pass a specific version
prompt = client.api.get_prompt(name="RAG prompt")
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

const client = new LiteralClient(process.env['LITERAL_API_KEY']);

const promptName = 'RAG prompt';
// This will fetch the champion version, you can also pass a specific version
async function main() {
    const prompt = await client.api.getPrompt(promptName);
    console.log(prompt);
}
main();
```

</CodeGroup>

### Create a prompt

You can create a new prompt on the Literal AI platform using the [Prompt Playground](/concepts/prompt/prompt-playground), but you can also create one using the API. The name of the prompt is its ID, so you cannot change this afterwards.

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

# define prompt
PROMPT_NAME = "RAG prompt"
template_messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant that always answers questions. Keep it short, and if available prefer responding with code. You are talking with {{name}} who has the following interests: {{interests}}."
    },
    {
        "role": "user",
        "content": "Answer the question based on the context below.\nContext:\n{{#context}}\n{{.}}\n{{/context}}\nQuestion:\n{{question}}\nAnswer:"
    }
]

# create the prompt, send to Literal AI
prompt = literal_client.api.get_or_create_prompt(name=PROMPT_NAME, template_messages=template_messages)

```

```typescript TypeScript
import { IGenerationMessage, LiteralClient } from '@literalai/client';

const client = new LiteralClient(process.env['LITERAL_API_KEY']);

async function main() {
    // define the prompt
    const PROMPT_NAME = 'RAG prompt';

    const promptTemplate = [
        {
            "role": "system",
            "content": "You are a helpful assistant that always answers questions. Keep it short, and if available prefer responding with code. You are talking with {{name}} who has the following interests: {{interests}}."
        },
        {
            "role": "user",
            "content": "Answer the question based on the context below.\nContext:\n{{#context}}\n{{.}}\n{{/context}}\nQuestion:\n{{question}}\nAnswer:"
        }
    ]

    const prompt = await client.api.getOrCreatePrompt(
        PROMPT_NAME,
        promptTemplate as IGenerationMessage[]
    );
}

main();
```
</CodeGroup>

### Format a Prompt

Once you got your prompt, you can format it to get messages in the OpenAI format.

Combining prompts with integrations (like the [OpenAI integration](/integrations/openai)) allows you to log the generations and to track which prompt versions were used to generate them.

You format messages like this:

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

# pull the prompt from Literal AI
prompt = literal_client.api.get_prompt(name="RAG prompt")

# format the prompt with a variable
query = "What are the features of Literal AI?"
variables = {"question": query}
messages = prompt.format_messages(variables)
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

const client = new LiteralClient(process.env['LITERAL_API_KEY']);

async function main() {
    // pull the prompt from Literal AI
    const prompt = await client.api.getPrompt('RAG prompt');

    // format the prompt with a variable
    const variables = { query: 'What are the features of Literal AI?' };
    const messages = prompt.formatMessages(variables);
}

main();
```
</CodeGroup>

<Warning>
Make sure to always prepare your messages with `Prompt.format_messages` &ndash; or its
TypeScript equivalent, even if you do not have variables to resolve. This ensures
messages are logged properly when using our OpenAI instrumentation.
</Warning>

### Langchain Chat Template

Since Langchain has a different format for prompts, you can convert a Literal AI prompt to a Langchain Chat Template.

You can combine the prompt with the [Langchain integration](/integrations/langchain) to log the generations and to track which prompt versions were used to generate them.

More explanation and example code can be found in [this guide](/guides/prompt/prompt-langchain).

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

# pull the prompt from Literal AI
prompt = literal_client.api.get_prompt(name="RAG prompt")

# convert to LangChain prompt
langchain_prompt = prompt.to_langchain_chat_prompt_template()
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

const client = new LiteralClient(process.env['LITERAL_API_KEY']);

// pull the prompt from Literal AI
const prompt = await client.api.getPrompt('RAG prompt');

// convert to LangChain prompt
const chatPrompt = prompt.toLangchainChatPromptTemplate();

```
</CodeGroup>