---
title: "Prompt"
---

You can view your prompts in the Prompt Management page. This overview shows the prompts that you have created, manage different versions of these prompts and lets you create and test prompt templates in collaboration with your teammates. In the [Literal Prompt Playground](#prompt-playground), which is connected to prompts and threads, you can debug prompts in-context, draft and iterates on prompts. Once you are happy with your prompt, you can save it and use it in your code.

Prompts are managed in the Literal platform, which has several benefits:
- Non technical people can draft and iterate on prompts.
- You can deploy a new version of your prompt without deploying your code.
- You can track which prompt version for a specific generation.
- You can compare prompt versions to see which one performs better.


# Prompt Playground

The playground is an environment in which you can create, test, and debug prompts. You can access the playground via
1. **The main menu**. This allows you to create, test and save prompts from scratch.
2. **Prompts**. This allows you to view, edit and test your prompt template and LLM settings and manage prompt versioning.
3. **LLM Generations**. Via Generations you can access the playground with an individual generation from a real chat conversation. This allows you to debug a single generation of the LLM in context.

## Design prompt templates

On the left hand side of the view you can create prompt templates. In a `system message` you give instructions about the role of the assistant. For example, you can say that the assistant should provide concise answers. 

<Frame caption="Playground Prompt Template">
  <img src="/images/playground-prompt-template.png" alt="An example of a prompt template in the playground." />
</Frame>


## Get a Prompt

Getting a prompt is the first step to use it in your code. You can get a prompt by its name.

<CodeGroup>
```python Python
import os
from literalai import LiteralClient

client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
# This will fetch the champion version, you can also pass a specific version
prompt = await client.api.get_prompt(name="Default")

# Synchronous version
prompt = client.api.get_prompt_sync(name="Default")
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

const client = new LiteralClient(process.env['LITERAL_API_KEY']);

const promptName = 'Default';
// This will fetch the champion version, you can also pass a specific version
const prompt = await client.api.getPrompt(promptName);
```

</CodeGroup>

## Format a Prompt

Once you got your prompt, you can format it to get messages in the OpenAI format.

Combining prompts with integrations (like the [OpenAI integration](/integrations/openai)) allows you to log the generations and to track which prompt versions were used to generate them.

<CodeGroup>
```python Python
import os
import asyncio
from literalai import LiteralClient
from openai import AsyncOpenAI

openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
# Optionally instrument the openai client to log generations
literal_client.instrument_openai()

async def main():
    prompt = await literal_client.api.get_prompt(name="Default")

    # Optionally pass variables to the prompt
    variables = {"foo": "bar"}

    messages = prompt.format(variables)
    stream = await openai_client.chat.completions.create(
        messages=messages,
        # Optionally pass the tools defined in the prompt
        tools=prompt.tools,
        # Pass the settings defined in the prompt
        **prompt.settings,
        stream=True
    )

    async for chunk in stream:
        print(chunk)

loop = asyncio.get_event_loop()
loop.run_until_complete(main())
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';
import OpenAI from 'openai';

const literalClient = new LiteralClient(process.env['LITERAL_API_KEY']);

const openai = new OpenAI({
  apiKey: process.env['OPENAI_API_KEY'],
});

async function main() {
  const prompt = await literalClient.api.getPrompt('Default');

  // Optionally pass variables to the prompt
  const variables = { foo: 'bar' };

  const messages = prompt.format(variables);

  const result = await openai.chat.completions.create({
    ...prompt.settings,
    tools: prompt.tools,
    messages: messages,
    stream: true
  });

  await client.instrumentation.openai(result);
}

main();

```

</CodeGroup>

## Langchain Chat Template

Since Langchain has a different format for prompts, you can convert a Literal prompt to a Langchain Chat Template.

You can combine the prompt with the [Langchain integration](/integrations/langchain) to log the generations and to track which prompt versions were used to generate them.

<CodeGroup>
```python Python
import os
import asyncio
from literalai import LiteralClient

from langchain_openai import ChatOpenAI
from langchain.schema.runnable.config import RunnableConfig
from langchain.schema import StrOutputParser

literal_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))

async def main():
    prompt = await literal_client.api.get_prompt(name="Default")
    lc_prompt = prompt.to_langchain_chat_prompt_template()

    model = ChatOpenAI(streaming=True)
    runnable = lc_prompt | model | StrOutputParser()
    
    cb = literal_client.langchain_callback()
    variables = {"foo": "bar"}

    res = runnable.invoke(
        variables, 
        config=RunnableConfig(callbacks=[cb], run_name="Test run")
        )

loop = asyncio.get_event_loop()
loop.run_until_complete(main())
```

```typescript TypeScript
import { LiteralClient } from '@literalai/client';

import { StringOutputParser } from '@langchain/core/output_parsers';
import { ChatOpenAI } from '@langchain/openai';

const literalClient = new LiteralClient(process.env['LITERAL_API_KEY']);

async function main() {
  const prompt = await literalClient.api.getPrompt('Default');

  const chatPrompt = prompt.toLangchainChatPromptTemplate();

  const model = new ChatOpenAI({});
  const outputParser = new StringOutputParser();

  const chain = chatPrompt.pipe(model).pipe(outputParser);

  const variables = { foo: 'bar' };

  const response = await chain.invoke(
    variables,
    {
      runName: 'Test run',
      callbacks: [client.instrumentation.langchain.literalCallback()]
    }
  );
}

main();

```

</CodeGroup>