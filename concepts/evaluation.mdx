---
title: "Evaluation"
---

## Introduction

Evaluation is key to enable continuous deployment of LLM-based applications and guarantee that newer
versions perform better than previous ones. To best capture the **user experience** one must understand the
multiple steps which make up the application. As AI applications grow in complexity, they tend to chain 
multiple steps.

Literal decorators let you catch the various steps of an AI application and gather the inputs
and outputs of each. With those, one may evaluate the performance of their application at any level, 
building the most relevant KPIs for end-users:
- Thread &mdash; user satisfaction
- Agent Run &mdash; task completion
- LLM Generation &mdash; hallucination

An example is the vanilla Retrieval Augmented Generation (RAG), which augments Large Language Models
(LLMs) with domain-specific data. Examples of metrics you can score against are:
- context relevancy
- faithfulness
- answer relevancy
- etc.

<Frame caption="Vanilla RAG Thread">
  <img src="/images/rag-thread.png" alt="A thread on a RAG application" />
</Frame>

Literal offers a compelling user/developer interface to build and interact with datasets.
Through `Dataset` objects, users can iterate on an application parameter and view
its impact via Experiments. 

For instance, a developer can iterate on their prompt template and check the impact of their
modifications on production data. 

Evaluations can either take place:
- on a `Dataset` which contains a number of steps' inputs/outputs to score &mdash; these are **offline evaluations**
- on live production data &mdash; those are **continuous evaluations**


## Offline evaluations

With offline evaluations, developers can leverage their favorite framework, such as
promptfoo, Ragas, OpenAI Evals, LangChain Evaluators, etc.

Our [Prompt Iteration with Promptfoo](https://github.com/Chainlit/literal-cookbook/tree/main/evaluation/typescript/prompt-iteration-promptfoo)
cookbook shows how a developer can leverage the `promptfoo` library in TypeScript to
compute similarity scores and persist the results of their experiments directly on Literal. 

For Python aficionados, our [Context Relevancy with Ragas](https://github.com/Chainlit/literal-cookbook/tree/main/evaluation/python) notebook experiments on an example RAG
application to score context relevancy.

For more examples on evaluations, please browse our [Literal Cookbooks](https://github.com/Chainlit/literal-cookbook/tree/main/evaluation).

<Card title="Evaluation - Literal Cookbooks" icon="github" href="https://github.com/Chainlit/literal-cookbook/tree/main/evaluation">
Explore additional evaluation concepts and frameworks.
</Card>

## Continuous evaluations

Contact Us [here](https://cal.com/dan-constantini/15min) to know more about how we perform continuous
evaluations of LLM applications.
