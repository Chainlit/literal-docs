---
title: "Evaluation"
---

## Introduction

Evaluation is key to enable continuous deployment of LLM-based applications and guarantee that newer
versions perform better than previous ones. To best capture the **user experience** one must understand the
multiple steps which make up the application. As AI applications grow in complexity, they tend to chain 
multiple steps.

Literal lets you log & monitor the various steps of your LLM application. By doing so, you can continuously improve the performance of your LLM system, building the most relevant metrics:
- Thread &mdash; user satisfaction, etc.
- Agent Run &mdash; task completion, etc.
- LLM Generation &mdash; hallucination, etc.

An example is the vanilla Retrieval Augmented Generation (RAG), which augments Large Language Models
(LLMs) with domain-specific data. Examples of metrics you can score against are:
- context relevancy
- faithfulness
- answer relevancy
- etc.

<Frame caption="Vanilla RAG Thread">
  <img src="/images/rag-thread.png" alt="A thread on a RAG application" />
</Frame>

Literal offers a compelling user/developer interface to build and interact with datasets.
Through `Dataset` objects, users can iterate on a parameter of the LLM system - such as prompt template, LLM provider, temperature, etc - and view its impact via Experiments. 

A developer can also iterate on their prompt template and check the impact of their
modifications on production data. 

Evaluations can either take place:
- on a `Dataset` which contains a number of steps' inputs/outputs to score &mdash; these are **offline evaluations**
- on live production data &mdash; those are **continuous evaluations**


## Offline evaluations

With offline evaluations, developers can leverage their favorite framework, such as
promptfoo, Ragas, OpenAI Evals, LangChain Evaluators, etc.

- [Prompt Iteration with **Promptfoo**](https://github.com/Chainlit/literal-cookbook/tree/main/evaluation/typescript/prompt-iteration-promptfoo):
Our cookbook shows how a developer can leverage the `promptfoo` library in TypeScript to
compute similarity scores and persist the results of their experiments directly on Literal. 
- [Context Relevancy with **Ragas**](https://github.com/Chainlit/literal-cookbook/tree/main/evaluation/python): our notebook experiments on an example RAG
application to score context relevancy.

For more examples on evaluations, please browse our [Literal Cookbooks](https://github.com/Chainlit/literal-cookbook/tree/main/evaluation).

<Card title="Evaluation - Literal Cookbooks" icon="github" href="https://github.com/Chainlit/literal-cookbook/tree/main/evaluation">
Explore additional evaluation concepts and frameworks.
</Card>

## Continuous evaluations

Contact Us [here](https://cal.com/dan-constantini/15min) to know more about how we perform continuous
evaluations of LLM applications.
