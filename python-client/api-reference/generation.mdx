---
title: Generation
---

The following objects can be passed to a [step](/concepts/observability/step)

## ChatGeneration

```python
@dataclass
class ChatGeneration(BaseGeneration):
    messages: List[GenerationMessage] = Field(default_factory=list)
    type = GenerationType.CHAT
```

<ParamField path="messages" type="List[Dict]">
  List of messages sent to the LLM. Following the openai chat format.
</ParamField>

<ParamField path="provider" type="str" optional>
  The provider of the LLM, like `openai`.
</ParamField>

<ParamField path="model" type="str" optional>
  The model used for the generation, like `gpt-4`.
</ParamField>

<ParamField path="error" type="str" optional>
  The error message if the generation failed.
</ParamField>

<ParamField path="settings" type="Dict" optional>
  The settings of the LLM, like `temperature`...
</ParamField>

<ParamField path="variables" type="Dict" optional>
  The variables used to format the prompt.
</ParamField>

<ParamField path="tags" type="List[str]" optional>
  Optional tags to add to the generation.
</ParamField>

<ParamField path="prompt" type="str" optional>
  The prompt used for the generation.
</ParamField>

<ParamField path="tools" type="Dict" optional>
  The tools used to generate the completion, following the openai format.
</ParamField>

<ParamField path="message_completion" type="Dict" optional>
  The message returned by the LLM, following the openai chat format.
</ParamField>

<ParamField path="token_count" type="int" optional>
  The token count of the completion.
</ParamField>

<ParamField path="input_token_count" type="int" optional>
  The token count of the input.
</ParamField>

<ParamField path="output_token_count" type="int" optional>
  The token count of the output.
</ParamField>

<ParamField path="tt_first_token" type="float" optional>
  The time it took to generate the first token. Only available when streaming.
</ParamField>

<ParamField path="token_throughput_in_s" type="float" optional>
  The token throughput in tokens per second. Only available when streaming.
</ParamField>

<ParamField path="duration" type="float" optional>
  The duration of the generation in ms.
</ParamField>

## CompletionGeneration

<ParamField path="provider" type="str" optional>
  The provider of the LLM, like `openai`.
</ParamField>

<ParamField path="model" type="str" optional>
  The model used for the generation, like `gpt-4`.
</ParamField>

<ParamField path="error" type="str" optional>
  The error message if the generation failed.
</ParamField>

<ParamField path="settings" type="Dict" optional>
  The settings of the LLM, like `temperature`...
</ParamField>

<ParamField path="variables" type="Dict" optional>
  The variables used to format the prompt.
</ParamField>

<ParamField path="tags" type="List[str]" optional>
  Optional tags to add to the generation.
</ParamField>

<ParamField path="prompt" type="str" optional>
  The prompt used for the generation.
</ParamField>

<ParamField path="completion" type="str" optional>
  The completion returned by the LLM.
</ParamField>

<ParamField path="token_count" type="int" optional>
  The token count of the completion.
</ParamField>

<ParamField path="input_token_count" type="int" optional>
  The token count of the input.
</ParamField>

<ParamField path="output_token_count" type="int" optional>
  The token count of the output.
</ParamField>

<ParamField path="tt_first_token" type="float" optional>
  The time it took to generate the first token. Only available when streaming.
</ParamField>

<ParamField path="token_throughput_in_s" type="float" optional>
  The token throughput in tokens per second. Only available when streaming.
</ParamField>

<ParamField path="duration" type="float" optional>
  The duration of the generation in ms.
</ParamField>

